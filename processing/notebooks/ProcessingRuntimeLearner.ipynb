{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy import stats\n",
    "import itertools as it\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as selection\n",
    "import os\n",
    "import json\n",
    "import pickle as pic\n",
    "import importlib\n",
    "import sys\n",
    "sys.path.append(\"./../utils\")\n",
    "sys.path.append(\"./../sklearn\")\n",
    "import util\n",
    "import constants as const\n",
    "import ColumnTransformer\n",
    "importlib.reload(util)\n",
    "importlib.reload(const)\n",
    "importlib.reload(ColumnTransformer)\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('precision', 6)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBRegressor\n",
    "import json\n",
    "import pickle\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Some Constants and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_partitioners = [\n",
    "    '1dd', \n",
    "    '1ds', \n",
    "    '2d', \n",
    "    '2ps', \n",
    "    'crvc', \n",
    "    'dbh', \n",
    "    'hdrf',\n",
    "    'hep1',\n",
    "    'hep10', \n",
    "    'hep100', \n",
    "    'ne',\n",
    "] \n",
    "\n",
    "\n",
    "TARGET = \"processing_time\"\n",
    "target = \"processing_time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, we make copies. We will later filter. \n",
    "X_train = pd.read_csv(\"../datasets/graph-processing-run-time_train.csv\")\n",
    "y_train = X_train.copy()\n",
    "# Yes, we make copies. We will later filter. \n",
    "X_test = pd.read_csv(\"../datasets/graph-processing-run-time_test.csv\")\n",
    "y_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper\n",
    "## 2.1 For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, algorithm, model, X_test, y_test, target):\n",
    "    \"\"\" We evalute the given model on the testset. \n",
    "\n",
    "    Args:\n",
    "        model_name (string): the name of the model, e.g., XGB for extrem gradient boosting\n",
    "        algorithm (string): the graph processing algorithm, e.g., cc, pr, sssp1, ...\n",
    "        model (model): the trained model\n",
    "        X_test (test data features): the testdata\n",
    "        y_test (test date target): the testdata\n",
    "        features (list): the features used in the model\n",
    "        target (string): the name if the target column\n",
    "    Returns:\n",
    "        list of dicts: the scores evaluated by partitioner and by graph \n",
    "    \"\"\"\n",
    "    \n",
    "    graphs = X_test.graph.unique()\n",
    "    partitioners = X_test.partitioner.unique()\n",
    "\n",
    "    results = []\n",
    "    # 1. Get scores by partitioner and per test graph. This for loop also works if we only have on partitioner.\n",
    "    for graph in graphs:\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.graph == graph)  & (_X_test.partitioner == partitioner) ]\n",
    "            _y_test = _y_test[(_y_test.graph == graph) & (_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted processing time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # The true processing time\n",
    "            _X_test[target] = _y_test[target]\n",
    "\n",
    "            # Get the metrics/scores.\n",
    "            metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "            \n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = graph.split(\"/\")[-1]\n",
    "            result[\"model\"] = model_name\n",
    "            result[\"algorithm\"] = algorithm\n",
    "            results.append(result)\n",
    "\n",
    "    # Since we also use this evaluation method for a model trained only for one single partitioner. \n",
    "    # In the next line we will evaluate how well the model performed for the different partitioners on the given graph processing algorithm: One model multiple partitioners and one graph processing algorithm.\n",
    "    if (len(partitioners) > 1):\n",
    "        # 2. Get result by partitioner and per test grap\n",
    "        for partitioner in partitioners:\n",
    "            _X_test = X_test.copy()\n",
    "            _y_test = y_test.copy()\n",
    "            \n",
    "            # Only evaluate the current graph on the current partitioner\n",
    "            _X_test = _X_test[(_X_test.partitioner == partitioner)]\n",
    "            _y_test = _y_test[(_y_test.partitioner == partitioner)]\n",
    "\n",
    "            # The predicted processing time\n",
    "            _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "\n",
    "            # The true processing time\n",
    "            _X_test[target] = _y_test[target]\n",
    "\n",
    "            metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "            result = metrics.get_metrics(_X_test)\n",
    "\n",
    "            result[\"partitioner\"] = partitioner\n",
    "            result[\"graph\"] = \"all\"\n",
    "            result[\"model\"] = model_name\n",
    "            result[\"algorithm\"] = algorithm\n",
    "            results.append(result)\n",
    "\n",
    "    \n",
    "        # 3. Get the overalle score for all partitoiners on all graphs for the fiven graph processing algorithm\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "    \n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = \"all\"\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        result[\"algorithm\"] = algorithm\n",
    "        results.append(result)\n",
    "    \n",
    "    #IF We only train with one partitioner, then we can not set partitioner to \"all\"\n",
    "    else:\n",
    "        # 3. Get for the one partitioner the scores. In this else we only have one partitioner which will be at index 0\n",
    "        partitioner = partitioners[0]\n",
    "        _X_test = X_test.copy()\n",
    "        _y_test = y_test.copy()\n",
    "    \n",
    "        _X_test[\"predicted_{}\".format(target)] = model.predict(_X_test)\n",
    "        _X_test[target] = _y_test[target]\n",
    "\n",
    "\n",
    "        metrics = helper.Metrics(target, \"predicted_{}\".format(target))\n",
    "        result = metrics.get_metrics(_X_test)\n",
    "        result[\"partitioner\"] = partitioner\n",
    "        result[\"graph\"] = \"all\"\n",
    "        result[\"model\"] = model_name\n",
    "        result[\"algorithm\"] = algorithm\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name,algorithm, model, X_train, y_train, X_test, y_test, feature_combinations, target, store_model=False, result_file_name=\"\"):\n",
    "    print(\"Trainer will store model in \", result_file_name)\n",
    "    \"\"\"Traing the given pipeline on the training data and evaluate the model on the test set. \n",
    "\n",
    "    Args:\n",
    "        model_name (string): name of the model, e.g., rfr for random forest regressor\n",
    "        model (pipeline): the pipeline to traing\n",
    "        X_train (_type_): train data - features\n",
    "        y_train (_type_): train data - target\n",
    "        X_test (_type_): test data - features\n",
    "        y_test (_type_): test data - target\n",
    "        target (_type_): the target to predict\n",
    "\n",
    "    Returns:\n",
    "        list: the achieved scores\n",
    "    \"\"\"\n",
    "    _X_train = X_train.copy()\n",
    "    _y_train = y_train.copy()\n",
    "    _X_test = X_test.copy()\n",
    "    _y_test = y_test.copy()\n",
    "\n",
    "    model.fit(_X_train, _y_train[target])\n",
    "    print(\"PARAMETERS\", model.get_params().keys())\n",
    "    print(\"Best params\", model.best_params_)\n",
    "    print(\"Mean Test score\", model.cv_results_[\"mean_test_score\"])\n",
    "    \n",
    "    # Now we have the trained model\n",
    "    # We store all the cross validation scores and also the best parameters\n",
    "    # We further directly evaluation on the test set and also store these scores. \n",
    "    # IMPORTANT: Later on, we need to make the model selection based on the crossvalidation scores, not on the test set. \n",
    "\n",
    "    # We serialize and store the best model     \n",
    "    if (store_model and not \"\" == result_file_name):\n",
    "        pickle.dump(model, open(result_file_name, 'wb'))\n",
    "\n",
    "    # Get the scores. \n",
    "    scores = evaluate_model(model_name, algorithm, model, X_test=_X_test, y_test=_y_test, target=target)\n",
    "    # Yes, for each score we add the cv score on the rmat graphs, the best hyperparameters. \n",
    "    # Because even if we evaluate on different graphs/partitioners, these score do not differ because they are based on synthetic graphs and lead to the model selection  \n",
    "    for score in scores:\n",
    "        score[\"rmat_best_params\"] = model.best_params_\n",
    "        score[\"rmat_cv_score\"] =  model.cv_results_[\"mean_test_score\"][model.best_index_]\n",
    "        score[\"rmat_cv_scores\"] =  model.cv_results_[\"mean_test_score\"]\n",
    "        score[\"rmat_best_index\"] = model.best_index_\n",
    "    return scores\n",
    "   # print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training used features\n",
    "## 3.1 Used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our features set consits of 32  combinations\n",
      "features:  [['num_edges', 'num_vertices'], ['vertex_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'num_edges', 'num_vertices'], ['source_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'source_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'source_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'source_balance', 'num_edges', 'num_vertices'], ['replication_factor', 'num_edges', 'num_vertices'], ['vertex_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['destination_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['source_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['vertex_balance', 'source_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['destination_balance', 'source_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'num_edges', 'num_vertices'], ['edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['source_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'source_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'source_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'source_balance', 'edge_balance', 'num_edges', 'num_vertices'], ['replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices']]\n"
     ]
    }
   ],
   "source": [
    "graph_properties_simple = [\n",
    "  'num_edges',\n",
    "  'num_vertices',\n",
    "]\n",
    "\n",
    "featuresets = []\n",
    "for graph_properties in [\n",
    "  graph_properties_simple, \n",
    "  ]:\n",
    "  quality_features = helper.create_Feature_combinations(\n",
    "    [\n",
    "      \"vertex_balance\",\t\n",
    "      \"destination_balance\",\t\n",
    "      \"source_balance\",\t\n",
    "      \"replication_factor\",\n",
    "      \"edge_balance\",\n",
    "    ], \n",
    "    [])   \n",
    "  for quality_feature in quality_features:\n",
    "    features = quality_feature + graph_properties\n",
    "    featuresets.append(features)\n",
    "print(\"Our features set consits of\", len(featuresets), \" combinations\\nfeatures: \", featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We start the training with model (PolyRegression) for algorithm (cc). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_cc_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1960s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1488s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  98 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1647s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 244 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 289 out of 320 | elapsed:    3.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.24698655 -0.24861785 -0.24556513 -0.24125816 -0.24512154 -0.24166182\n",
      " -0.25629472 -0.2494101  -0.24916766 -0.24580571 -0.25503581 -0.24295386\n",
      " -0.25143868 -0.24952429 -0.26631155 -0.24998076 -0.27179757 -0.24616139\n",
      " -0.27465066 -0.22608927 -0.26948967 -0.22141319 -0.28181647 -0.2167848\n",
      " -0.26941256 -0.23668552 -0.27873876 -0.22311221 -0.27334321 -0.22102895\n",
      " -0.28674842 -0.2128196  -0.2498023  -0.24865188 -0.24792253 -0.24557327\n",
      " -0.2482949  -0.24609115 -0.25814547 -0.25272512 -0.25310013 -0.25106119\n",
      " -0.25779745 -0.24785262 -0.25467943 -0.25549615 -0.26800951 -0.25485012\n",
      " -0.27317792 -0.22998806 -0.27813905 -0.21130931 -0.27207318 -0.20354887\n",
      " -0.28379634 -0.20352407 -0.27346674 -0.2206049  -0.28146295 -0.20765729\n",
      " -0.27570693 -0.20067403 -0.28764793 -0.19910979]\n",
      "We start the training with model (MLP) for algorithm (cc). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_cc_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   32.8s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   40.9s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   46.8s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   50.1s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   56.7s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.99776274 -0.97233731 -0.89265025 -0.50336543 -0.97213331 -0.93133414\n",
      " -0.4142786  -0.98885528 -0.95775273 -0.9132238  -0.64730511 -0.98185286\n",
      " -0.92085186 -0.58446764 -0.98871496 -0.95766706 -0.90771434 -0.64701545\n",
      " -0.98181283 -0.9209005  -0.577301   -0.98753452 -0.97781347 -0.91937307\n",
      " -0.6753815  -0.97422491 -0.92053524 -0.48407826 -0.98892883 -0.95800319\n",
      " -0.90506348 -0.64799262 -0.98212542 -0.92132873 -0.57590863 -0.98744723\n",
      " -0.97718644 -0.91531796 -0.66106847 -0.97418015 -0.91423862 -0.46678141\n",
      " -0.9892948  -0.97733015 -0.91710361 -0.66264034 -0.97376097 -0.91455845\n",
      " -0.46675152 -0.98345225 -0.94781504 -0.79966038 -0.61572012 -0.98469571\n",
      " -0.91618471 -0.38614741 -0.98860052 -0.95942645 -0.92044867 -0.64253659\n",
      " -0.98109087 -0.92230467 -0.58205172 -0.98878641 -0.97864358 -0.92152483\n",
      " -0.6834851  -0.97488482 -0.9262582  -0.48436286 -0.988671   -0.97855878\n",
      " -0.92097714 -0.68226659 -0.97465538 -0.92601258 -0.4832268  -0.98653504\n",
      " -0.95620997 -0.81716927 -0.67701371 -0.98109741 -0.93334225 -0.43175439\n",
      " -0.98868004 -0.9786087  -0.92057524 -0.68039108 -0.97449106 -0.92585321\n",
      " -0.47948068 -0.98651995 -0.95617653 -0.81664856 -0.66432583 -0.98102353\n",
      " -0.93313157 -0.42947481 -0.98641862 -0.9558224  -0.816424   -0.66184823\n",
      " -0.98100734 -0.93336495 -0.42605309 -0.98489858 -0.93139454 -0.80723221\n",
      " -0.53440522 -0.97814016 -0.89653696 -0.42141154 -0.98830645 -0.96486393\n",
      " -0.9097397  -0.64437651 -0.98121456 -0.91145839 -0.58150344 -0.98927507\n",
      " -0.98144527 -0.92896544 -0.7087022  -0.97680019 -0.92934593 -0.50723066\n",
      " -0.98917798 -0.98148444 -0.92920549 -0.705775   -0.9767407  -0.92966255\n",
      " -0.5035775  -0.98758684 -0.95953676 -0.83498621 -0.71965966 -0.98337032\n",
      " -0.94016701 -0.44327123 -0.98920584 -0.9815749  -0.92945991 -0.7107762\n",
      " -0.97681679 -0.92977451 -0.50138552 -0.98759427 -0.95931959 -0.83401812\n",
      " -0.70885347 -0.98321843 -0.94025766 -0.43245997 -0.98769947 -0.95940145\n",
      " -0.8345874  -0.70822234 -0.98333875 -0.94075881 -0.43188165 -0.98635979\n",
      " -0.93657774 -0.8184728  -0.53568706 -0.97954358 -0.90707444 -0.37170809\n",
      " -0.98973415 -0.98027253 -0.92692372 -0.70810566 -0.97572389 -0.92667246\n",
      " -0.5081136  -0.98891235 -0.96453419 -0.84780224 -0.71463152 -0.98717853\n",
      " -0.93835826 -0.43744774 -0.98887209 -0.96541008 -0.84814575 -0.71390649\n",
      " -0.98705014 -0.93849316 -0.43441681 -0.98771684 -0.94321053 -0.82196607\n",
      " -0.55978564 -0.98171349 -0.92182498 -0.41452403 -0.98879207 -0.9627666\n",
      " -0.84737045 -0.71427169 -0.98789814 -0.93839921 -0.43802976 -0.98772352\n",
      " -0.94277515 -0.82100318 -0.55419268 -0.98174165 -0.91969372 -0.41586029\n",
      " -0.98756696 -0.94281437 -0.81988865 -0.55119165 -0.98180432 -0.91975592\n",
      " -0.41345017 -0.98683167 -0.95933156 -0.82187602 -0.52600231 -0.97903218\n",
      " -0.88952212 -0.41285813]\n",
      "We start the training with model (PolyRegression) for algorithm (pr). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_pr_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0300s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0300s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1924s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1755s.) Setting batch_size=16.\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 248 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 out of 320 | elapsed:    0.9s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 289 out of 320 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.20097273 -0.20198153 -0.19921569 -0.19716403 -0.19951049 -0.19839645\n",
      " -0.2008218  -0.20294515 -0.20661639 -0.19874576 -0.20807634 -0.19613125\n",
      " -0.20682185 -0.19485421 -0.20971923 -0.19815815 -0.21661472 -0.19685035\n",
      " -0.2135838  -0.17867974 -0.21026745 -0.17428536 -0.21055972 -0.17318004\n",
      " -0.2138145  -0.18897231 -0.21699212 -0.17401037 -0.21365635 -0.16653785\n",
      " -0.21506143 -0.16594136 -0.20037628 -0.20242319 -0.19804422 -0.19987711\n",
      " -0.19833257 -0.20139895 -0.19955905 -0.20609504 -0.20564329 -0.20324582\n",
      " -0.20717885 -0.19977514 -0.20607904 -0.19897755 -0.20889243 -0.20251876\n",
      " -0.21853183 -0.19523672 -0.21609249 -0.17702105 -0.21263787 -0.17220464\n",
      " -0.21262556 -0.17079805 -0.21661912 -0.18749532 -0.21924412 -0.17251276\n",
      " -0.21582559 -0.16255176 -0.21696877 -0.16329801]\n",
      "We start the training with model (MLP) for algorithm (pr). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_pr_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   41.6s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['edge_balance', 'num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.99294866 -0.91475782 -0.68365489 -0.30489297 -0.91613227 -0.79653005\n",
      " -0.30630299 -0.96385177 -0.8662375  -0.72756431 -0.34651815 -0.94302182\n",
      " -0.75465254 -0.29813966 -0.96327774 -0.86593712 -0.71699372 -0.34694813\n",
      " -0.94277295 -0.75431963 -0.29483919 -0.95972663 -0.92882    -0.75422271\n",
      " -0.39251777 -0.91704303 -0.76413771 -0.32151529 -0.96424108 -0.86714691\n",
      " -0.71314802 -0.34371855 -0.94400037 -0.7552457  -0.29272964 -0.95975695\n",
      " -0.92718845 -0.741521   -0.37595992 -0.91580104 -0.74179399 -0.3138097\n",
      " -0.96099317 -0.92736786 -0.7475786  -0.37980842 -0.91577504 -0.74159671\n",
      " -0.31299278 -0.94754725 -0.84521369 -0.50779558 -0.35555633 -0.9511492\n",
      " -0.76367933 -0.34743425 -0.96435552 -0.87358962 -0.75841017 -0.35362814\n",
      " -0.94183784 -0.76370648 -0.32587791 -0.96506793 -0.93150905 -0.7598559\n",
      " -0.41253008 -0.92210188 -0.77407177 -0.35983238 -0.96456247 -0.93137064\n",
      " -0.75922166 -0.41038569 -0.92167453 -0.77426223 -0.35776526 -0.95641314\n",
      " -0.85840952 -0.54015313 -0.41233929 -0.9400064  -0.7943579  -0.36582159\n",
      " -0.96469419 -0.93144435 -0.75655569 -0.41030435 -0.92147055 -0.77492234\n",
      " -0.35930688 -0.95626156 -0.85738322 -0.53459566 -0.41094732 -0.94001132\n",
      " -0.79411352 -0.36884694 -0.95592867 -0.85719061 -0.53222958 -0.40881247\n",
      " -0.93938221 -0.79283305 -0.37050369 -0.95158279 -0.79670193 -0.53229325\n",
      " -0.38575657 -0.93018838 -0.71190637 -0.36645801 -0.96277688 -0.88571562\n",
      " -0.72668887 -0.30650384 -0.94119963 -0.7407666  -0.2814924  -0.96490585\n",
      " -0.93964087 -0.78082923 -0.40001335 -0.92622939 -0.78223531 -0.29333943\n",
      " -0.96481448 -0.93976996 -0.78098799 -0.39932031 -0.92593066 -0.78382499\n",
      " -0.28859807 -0.95976845 -0.87137733 -0.56501616 -0.43158288 -0.94704515\n",
      " -0.81617744 -0.30247879 -0.96490568 -0.94003955 -0.78137515 -0.39454583\n",
      " -0.92648695 -0.7843431  -0.29105186 -0.96000553 -0.8719101  -0.55893657\n",
      " -0.41502447 -0.94621724 -0.81607918 -0.30029603 -0.96023089 -0.87184056\n",
      " -0.56067272 -0.41602434 -0.94639389 -0.81589575 -0.29895253 -0.95551483\n",
      " -0.80902553 -0.53905048 -0.33970982 -0.93494217 -0.73676498 -0.33319887\n",
      " -0.96624182 -0.93676185 -0.77697843 -0.41896652 -0.92349606 -0.77692046\n",
      " -0.32037428 -0.96349971 -0.88768444 -0.58260389 -0.42090391 -0.9577475\n",
      " -0.81040244 -0.3494547  -0.96319764 -0.88950165 -0.5834986  -0.41957336\n",
      " -0.95732331 -0.81039953 -0.34762067 -0.95952902 -0.82220842 -0.55368843\n",
      " -0.3836644  -0.9412458  -0.75404511 -0.35915229 -0.96316803 -0.88190022\n",
      " -0.58008616 -0.42396178 -0.96026807 -0.81009645 -0.34919262 -0.95931146\n",
      " -0.82032224 -0.54838365 -0.38420836 -0.9409297  -0.75233354 -0.36124329\n",
      " -0.95898412 -0.81972993 -0.54688278 -0.38174584 -0.94095887 -0.75140746\n",
      " -0.3615635  -0.95841042 -0.8712779  -0.56865384 -0.37487487 -0.9340907\n",
      " -0.70039294 -0.36383623]\n",
      "We start the training with model (PolyRegression) for algorithm (sssp1). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_sssp1_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0502s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1585s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 289 out of 320 | elapsed:    1.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.17071486 -0.17494824 -0.17006855 -0.17544827 -0.17079765 -0.17535652\n",
      " -0.17418293 -0.17713487 -0.17481526 -0.1788892  -0.17835746 -0.1720656\n",
      " -0.1779988  -0.18003227 -0.18002799 -0.17294359 -0.18565719 -0.16800245\n",
      " -0.18572581 -0.15435141 -0.1846425  -0.15687904 -0.18749484 -0.15449279\n",
      " -0.18229202 -0.16391816 -0.18935798 -0.14944201 -0.18807888 -0.15517246\n",
      " -0.1911327  -0.14992143 -0.17242925 -0.17583977 -0.17208874 -0.17725536\n",
      " -0.17310781 -0.17739153 -0.17586556 -0.17908591 -0.17796783 -0.18106009\n",
      " -0.18030651 -0.1740885  -0.1806047  -0.18220497 -0.18169212 -0.17518562\n",
      " -0.1876284  -0.16069933 -0.1878163  -0.14660856 -0.18639671 -0.14723345\n",
      " -0.18928552 -0.14664694 -0.18478828 -0.15796636 -0.19074376 -0.14111673\n",
      " -0.1897491  -0.14456785 -0.19205925 -0.14116277]\n",
      "We start the training with model (MLP) for algorithm (sssp1). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_sssp1_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   41.5s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   44.6s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   47.6s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   50.8s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.99931392 -0.99149632 -0.96567772 -0.81589929 -0.99173425 -0.97894614\n",
      " -0.75215711 -0.99639252 -0.98617328 -0.97096149 -0.86984696 -0.99437006\n",
      " -0.97339131 -0.83259794 -0.99639123 -0.98619247 -0.96805779 -0.86980935\n",
      " -0.99438241 -0.97313338 -0.83072442 -0.99598474 -0.99277584 -0.97302516\n",
      " -0.87547698 -0.99131662 -0.97315879 -0.75460556 -0.99645691 -0.98631713\n",
      " -0.96495372 -0.87026513 -0.99444963 -0.97311581 -0.83230152 -0.99594707\n",
      " -0.99257524 -0.97163269 -0.87287955 -0.99135151 -0.97116352 -0.74645003\n",
      " -0.99609936 -0.99235774 -0.97226758 -0.87388384 -0.99136889 -0.97124237\n",
      " -0.74916517 -0.99474961 -0.98281102 -0.93204949 -0.85231063 -0.99516018\n",
      " -0.97218529 -0.65511096 -0.99644079 -0.98694248 -0.9730324  -0.86528057\n",
      " -0.99420352 -0.97477283 -0.83686838 -0.99653984 -0.99299543 -0.97362622\n",
      " -0.88332005 -0.99213295 -0.97497354 -0.75790246 -0.99651768 -0.99299556\n",
      " -0.97353765 -0.88271461 -0.99214269 -0.97529306 -0.75851681 -0.99561648\n",
      " -0.98517267 -0.93788872 -0.87515673 -0.99385324 -0.97789892 -0.67096046\n",
      " -0.99652835 -0.99300373 -0.9734181  -0.88198061 -0.99211016 -0.97550456\n",
      " -0.75624805 -0.99561038 -0.9852201  -0.93775424 -0.87349442 -0.99383457\n",
      " -0.97771058 -0.65951756 -0.99560345 -0.98515748 -0.93778068 -0.87231829\n",
      " -0.99395472 -0.97741329 -0.65159541 -0.99508069 -0.97740608 -0.9342794\n",
      " -0.7934401  -0.99290664 -0.96591417 -0.60246489 -0.99632753 -0.98846744\n",
      " -0.96784467 -0.8652919  -0.99417625 -0.97083538 -0.83214423 -0.99647472\n",
      " -0.99384698 -0.97604157 -0.89311011 -0.99246949 -0.97609937 -0.78218749\n",
      " -0.99646594 -0.99388043 -0.97620173 -0.89321945 -0.992475   -0.97627789\n",
      " -0.78272013 -0.99591219 -0.98651415 -0.94373227 -0.89572557 -0.99457763\n",
      " -0.98040017 -0.71094427 -0.99648194 -0.99390616 -0.97618118 -0.89260027\n",
      " -0.99252023 -0.97624811 -0.7823496  -0.99596172 -0.98659864 -0.94366563\n",
      " -0.89466956 -0.99452733 -0.98019954 -0.70175191 -0.99599593 -0.98662508\n",
      " -0.94399313 -0.89444111 -0.99456365 -0.98030349 -0.7017788  -0.99549566\n",
      " -0.97915962 -0.93875994 -0.80350169 -0.99338337 -0.96924776 -0.63819767\n",
      " -0.99659532 -0.99356057 -0.97554592 -0.89185957 -0.99203308 -0.97548081\n",
      " -0.78163974 -0.99632491 -0.98796793 -0.94715987 -0.89592192 -0.99570996\n",
      " -0.97905686 -0.70694299 -0.9963112  -0.98859517 -0.9474866  -0.895968\n",
      " -0.99565425 -0.97899492 -0.70879526 -0.99590135 -0.9812807  -0.93843344\n",
      " -0.81445932 -0.99402895 -0.97342269 -0.65195132 -0.99629284 -0.98743189\n",
      " -0.9472751  -0.89613515 -0.9959191  -0.97898478 -0.70822446 -0.99585101\n",
      " -0.98125566 -0.93803358 -0.8129006  -0.99400215 -0.97325896 -0.6442681\n",
      " -0.99585915 -0.98120927 -0.93813644 -0.81292154 -0.99404599 -0.97324731\n",
      " -0.64247139 -0.99578308 -0.98709146 -0.93889539 -0.7792429  -0.99309045\n",
      " -0.96345004 -0.55290907]\n",
      "We start the training with model (PolyRegression) for algorithm (kcoreavg). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_kcoreavg_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0268s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0268s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 172 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.43433767 -0.45135163 -0.4331532  -0.45151606 -0.43517989 -0.45218274\n",
      " -0.43329846 -0.44516254 -0.43862589 -0.45882565 -0.44491732 -0.44917825\n",
      " -0.44169939 -0.46620108 -0.44139939 -0.44889031 -0.46242613 -0.42242885\n",
      " -0.47073383 -0.39919807 -0.46635891 -0.40614976 -0.4734267  -0.39520624\n",
      " -0.46174801 -0.41800142 -0.4741114  -0.39451857 -0.47031845 -0.40332612\n",
      " -0.47708189 -0.3881264  -0.43823087 -0.45286149 -0.4382304  -0.45694161\n",
      " -0.44046714 -0.4578334  -0.43736625 -0.45000321 -0.44469766 -0.46511294\n",
      " -0.4496824  -0.45623259 -0.44743593 -0.47427683 -0.44547199 -0.45516942\n",
      " -0.46251303 -0.40530652 -0.47163949 -0.37892251 -0.46766545 -0.38526772\n",
      " -0.47378945 -0.37580354 -0.4627695  -0.39520144 -0.47448278 -0.3711728\n",
      " -0.4715503  -0.38325536 -0.47648112 -0.36874111]\n",
      "We start the training with model (MLP) for algorithm (kcoreavg). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_kcoreavg_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.7s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.9s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   26.2s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   42.1s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   47.9s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   58.3s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.9987767  -0.98425197 -0.93832662 -0.68720135 -0.98461315 -0.96125838\n",
      " -0.58405092 -0.99356223 -0.97571764 -0.95218101 -0.78915368 -0.98983965\n",
      " -0.95419224 -0.73475315 -0.99357266 -0.9757716  -0.94713361 -0.7900607\n",
      " -0.98989349 -0.95442282 -0.72781697 -0.99309004 -0.98736622 -0.95426716\n",
      " -0.80097792 -0.98464422 -0.95224706 -0.6348093  -0.99362283 -0.97595125\n",
      " -0.94702893 -0.791993   -0.98994182 -0.95457071 -0.73488933 -0.99294242\n",
      " -0.98718711 -0.95230779 -0.79519812 -0.98476198 -0.95107817 -0.61812026\n",
      " -0.99360574 -0.98684112 -0.95306556 -0.79714895 -0.98477888 -0.95115027\n",
      " -0.61928638 -0.99066828 -0.97039639 -0.88595539 -0.75934663 -0.99148276\n",
      " -0.95291779 -0.51789987 -0.99331582 -0.97618701 -0.95762505 -0.79061559\n",
      " -0.98891574 -0.95709106 -0.74784071 -0.99355328 -0.9880002  -0.95695222\n",
      " -0.81710988 -0.98539876 -0.9589681  -0.64509267 -0.99355212 -0.98810993\n",
      " -0.95670927 -0.81438007 -0.98547963 -0.95877428 -0.64287766 -0.99271929\n",
      " -0.9758678  -0.89933955 -0.80063599 -0.98966688 -0.96295929 -0.54493741\n",
      " -0.99350628 -0.98808489 -0.95693396 -0.81465131 -0.98540677 -0.95874949\n",
      " -0.64181908 -0.99268104 -0.9758804  -0.89907232 -0.79613724 -0.9897216\n",
      " -0.96247693 -0.53669615 -0.99263357 -0.97561765 -0.89664147 -0.79369123\n",
      " -0.98972765 -0.96156925 -0.52974575 -0.99163334 -0.96147923 -0.8890262\n",
      " -0.68154951 -0.98791683 -0.94129466 -0.50307437 -0.99340034 -0.97914504\n",
      " -0.94879373 -0.77887493 -0.98948343 -0.94858632 -0.72736757 -0.99385608\n",
      " -0.98938303 -0.9600578  -0.82794428 -0.98668531 -0.96017125 -0.66119033\n",
      " -0.99388734 -0.98948379 -0.96057243 -0.82733021 -0.98683467 -0.96062489\n",
      " -0.66140972 -0.9928824  -0.97687392 -0.90676272 -0.83161845 -0.9910914\n",
      " -0.96620764 -0.59047574 -0.99385498 -0.98951802 -0.96064428 -0.83008278\n",
      " -0.98675726 -0.96061148 -0.66511656 -0.99306315 -0.97700733 -0.90661252\n",
      " -0.82819347 -0.99053423 -0.96654341 -0.57355601 -0.99317183 -0.97701664\n",
      " -0.90785985 -0.83011145 -0.99063947 -0.96688265 -0.56873851 -0.99226414\n",
      " -0.9639524  -0.89683245 -0.68793014 -0.98850124 -0.94714743 -0.50205833\n",
      " -0.99445113 -0.98957204 -0.96149641 -0.8431857  -0.9864493  -0.96164773\n",
      " -0.6868053  -0.99394889 -0.97996291 -0.91967653 -0.84399114 -0.99287349\n",
      " -0.96598744 -0.59919756 -0.99402289 -0.980659   -0.91950795 -0.84392315\n",
      " -0.99285712 -0.96591109 -0.5996665  -0.99338879 -0.96955611 -0.90221545\n",
      " -0.72328026 -0.99000233 -0.95827451 -0.52814343 -0.99390514 -0.97880926\n",
      " -0.91907092 -0.84320716 -0.99336163 -0.96591334 -0.60069673 -0.99322479\n",
      " -0.96946925 -0.90182037 -0.71635307 -0.98998972 -0.95821464 -0.52289316\n",
      " -0.99317073 -0.96935075 -0.9007241  -0.71673497 -0.99002486 -0.95820057\n",
      " -0.51820557 -0.99273851 -0.97675657 -0.90409124 -0.67572985 -0.98847095\n",
      " -0.93898078 -0.48526669]\n",
      "We start the training with model (PolyRegression) for algorithm (synthetic1c0). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_synthetic1c0_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0415s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1764s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 164 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 224 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 277 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 289 out of 320 | elapsed:    1.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.23595812 -0.24212823 -0.23820392 -0.24022621 -0.23882313 -0.24210347\n",
      " -0.24083839 -0.24886784 -0.24898148 -0.24004958 -0.25103753 -0.23852093\n",
      " -0.25062503 -0.23884001 -0.25364435 -0.24243497 -0.25723366 -0.21430502\n",
      " -0.25220354 -0.19183565 -0.24923456 -0.18836123 -0.24894897 -0.18758442\n",
      " -0.25114903 -0.20355435 -0.25718622 -0.18719134 -0.25469582 -0.1796175\n",
      " -0.25529976 -0.17952152 -0.23439347 -0.24212528 -0.23508851 -0.24342951\n",
      " -0.23572817 -0.24609038 -0.23764349 -0.25242028 -0.24661539 -0.24610982\n",
      " -0.24893391 -0.24300566 -0.24840415 -0.24417493 -0.25134631 -0.24846881\n",
      " -0.2590178  -0.21334744 -0.25484228 -0.19015504 -0.251783   -0.18610466\n",
      " -0.25141409 -0.18513698 -0.25399635 -0.20270555 -0.25950159 -0.18547551\n",
      " -0.25720353 -0.17589053 -0.25771813 -0.17679804]\n",
      "We start the training with model (MLP) for algorithm (synthetic1c0). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_synthetic1c0_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.4s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   39.4s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   48.7s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   55.2s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['edge_balance', 'num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.9950775  -0.93834021 -0.77019871 -0.35000001 -0.93820016 -0.84883\n",
      " -0.34919203 -0.97477351 -0.90569746 -0.79952036 -0.43132132 -0.96015528\n",
      " -0.82314044 -0.3782977  -0.9745361  -0.90546771 -0.79577846 -0.43248709\n",
      " -0.95999482 -0.82291708 -0.37682993 -0.97322798 -0.94936257 -0.82458472\n",
      " -0.46314983 -0.93958915 -0.81948119 -0.36770651 -0.97505502 -0.90641598\n",
      " -0.79239836 -0.43647773 -0.96040114 -0.82396975 -0.36741795 -0.9726769\n",
      " -0.94353603 -0.81650289 -0.44296323 -0.93987208 -0.81206166 -0.36190111\n",
      " -0.97489472 -0.94442847 -0.81895388 -0.44525141 -0.93996004 -0.81295432\n",
      " -0.36280411 -0.96357307 -0.88813011 -0.61661697 -0.40780234 -0.96677668\n",
      " -0.82596013 -0.38835621 -0.97454368 -0.90961101 -0.83099036 -0.43304337\n",
      " -0.95777228 -0.83168358 -0.40399322 -0.97539177 -0.95287086 -0.83140578\n",
      " -0.47933972 -0.94474772 -0.84103906 -0.40886922 -0.97519206 -0.95276517\n",
      " -0.83053223 -0.47688367 -0.944633   -0.83999654 -0.40664166 -0.97112224\n",
      " -0.90343448 -0.6459628  -0.46692626 -0.95931765 -0.85615538 -0.41278431\n",
      " -0.97518154 -0.95306948 -0.83025244 -0.47070116 -0.94455331 -0.84131349\n",
      " -0.41115739 -0.97096367 -0.90369702 -0.63866527 -0.45883871 -0.9592723\n",
      " -0.85219632 -0.41596768 -0.97080356 -0.90337036 -0.63585586 -0.45666652\n",
      " -0.95946888 -0.85121916 -0.41524626 -0.96720548 -0.85387685 -0.62595479\n",
      " -0.40955284 -0.95239576 -0.78747381 -0.41364265 -0.97345838 -0.91629203\n",
      " -0.80359487 -0.37201435 -0.95797935 -0.81057605 -0.34086166 -0.97590317\n",
      " -0.958166   -0.84627627 -0.48881556 -0.94924252 -0.84657072 -0.36692287\n",
      " -0.97583048 -0.95823646 -0.84652684 -0.48897764 -0.94915369 -0.84696126\n",
      " -0.36287426 -0.97199011 -0.91020026 -0.67211277 -0.51079067 -0.96349955\n",
      " -0.87143131 -0.35967953 -0.97599758 -0.95868662 -0.84758366 -0.48535362\n",
      " -0.94930817 -0.84723756 -0.36295114 -0.97294167 -0.91255766 -0.66690998\n",
      " -0.49196987 -0.96305873 -0.8707628  -0.35547832 -0.97310522 -0.91245499\n",
      " -0.66784862 -0.4921086  -0.96317758 -0.8712692  -0.35622495 -0.96935521\n",
      " -0.86157302 -0.64209586 -0.37659147 -0.95473119 -0.80694582 -0.37055316\n",
      " -0.97670823 -0.95601734 -0.84589151 -0.51332564 -0.94741426 -0.84206798\n",
      " -0.39561203 -0.974931   -0.92173267 -0.69875513 -0.51005956 -0.97114687\n",
      " -0.86596477 -0.39384908 -0.97484474 -0.92239567 -0.69826711 -0.50852328\n",
      " -0.9710124  -0.86511275 -0.39156815 -0.97282642 -0.87801145 -0.65780003\n",
      " -0.41931268 -0.95981641 -0.83064357 -0.40277129 -0.97478703 -0.91543606\n",
      " -0.69756817 -0.50746588 -0.97287109 -0.86524895 -0.39812279 -0.97232089\n",
      " -0.8782504  -0.65442476 -0.41865126 -0.95960154 -0.82968504 -0.40510653\n",
      " -0.97220315 -0.87793595 -0.65259597 -0.41610744 -0.95948663 -0.8291765\n",
      " -0.40415172 -0.97159933 -0.9177196  -0.66480184 -0.4085228  -0.9542808\n",
      " -0.77575705 -0.41240498]\n",
      "We start the training with model (PolyRegression) for algorithm (synthetic10c0). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_synthetic10c0_PolyRegression\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0317s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0317s.) Setting batch_size=4.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1988s.) Setting batch_size=8.\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 289 out of 320 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__poli', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__poli__degree', 'estimator__regressor__poli__include_bias', 'estimator__regressor__poli__interaction_only', 'estimator__regressor__poli__order', 'estimator__regressor__regressor__copy_X', 'estimator__regressor__regressor__fit_intercept', 'estimator__regressor__regressor__n_jobs', 'estimator__regressor__regressor__normalize', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}\n",
      "Mean Test score [-0.24663722 -0.25271756 -0.24985868 -0.25010683 -0.25037988 -0.25222109\n",
      " -0.2526238  -0.25909079 -0.26124623 -0.24961365 -0.26346505 -0.24869349\n",
      " -0.26305466 -0.24929825 -0.26612904 -0.25296229 -0.27437493 -0.21885006\n",
      " -0.27008459 -0.19475924 -0.26784489 -0.19122967 -0.26725777 -0.19042556\n",
      " -0.2686034  -0.20551036 -0.27568714 -0.19008651 -0.27373759 -0.18220391\n",
      " -0.27443028 -0.18336444 -0.2461778  -0.25378643 -0.2472372  -0.25550288\n",
      " -0.24784407 -0.25850629 -0.24973112 -0.26498901 -0.25896659 -0.25921117\n",
      " -0.26160627 -0.25617018 -0.26103176 -0.25768466 -0.26405706 -0.26232359\n",
      " -0.27567061 -0.22016095 -0.27218599 -0.19507755 -0.26984566 -0.19077575\n",
      " -0.26924382 -0.1901079  -0.27081591 -0.20732925 -0.27763065 -0.19088279\n",
      " -0.2757205  -0.18126685 -0.27629223 -0.18292982]\n",
      "We start the training with model (MLP) for algorithm (synthetic10c0). We build ONE model for all partitioners: True\n",
      "Trainer will store model in  /home/ubuntu/cephstorage/graph-processing-run-time-prediction-models/2022-12-12-17:41:37/single-model-for-all-partitioners_synthetic10c0_MLP\n",
      "Fitting 5 folds for each of 224 candidates, totalling 1120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 113 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 189 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=-1)]: Done 210 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done 233 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:   26.1s\n",
      "[Parallel(n_jobs=-1)]: Done 281 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   30.8s\n",
      "[Parallel(n_jobs=-1)]: Done 333 tasks      | elapsed:   33.6s\n",
      "[Parallel(n_jobs=-1)]: Done 360 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 389 tasks      | elapsed:   39.1s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 449 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed:   48.3s\n",
      "[Parallel(n_jobs=-1)]: Done 513 tasks      | elapsed:   51.8s\n",
      "[Parallel(n_jobs=-1)]: Done 546 tasks      | elapsed:   55.0s\n",
      "[Parallel(n_jobs=-1)]: Done 581 tasks      | elapsed:   58.7s\n",
      "[Parallel(n_jobs=-1)]: Done 616 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 653 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 690 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 729 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 809 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 850 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 893 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 936 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 981 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1026 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1073 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1120 out of 1120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS dict_keys(['cv', 'error_score', 'estimator__check_inverse', 'estimator__func', 'estimator__inverse_func', 'estimator__regressor__memory', 'estimator__regressor__steps', 'estimator__regressor__verbose', 'estimator__regressor__feature_selection', 'estimator__regressor__scaler', 'estimator__regressor__regressor', 'estimator__regressor__feature_selection__features', 'estimator__regressor__scaler__copy', 'estimator__regressor__scaler__with_mean', 'estimator__regressor__scaler__with_std', 'estimator__regressor__regressor__activation', 'estimator__regressor__regressor__alpha', 'estimator__regressor__regressor__batch_size', 'estimator__regressor__regressor__beta_1', 'estimator__regressor__regressor__beta_2', 'estimator__regressor__regressor__early_stopping', 'estimator__regressor__regressor__epsilon', 'estimator__regressor__regressor__hidden_layer_sizes', 'estimator__regressor__regressor__learning_rate', 'estimator__regressor__regressor__learning_rate_init', 'estimator__regressor__regressor__max_fun', 'estimator__regressor__regressor__max_iter', 'estimator__regressor__regressor__momentum', 'estimator__regressor__regressor__n_iter_no_change', 'estimator__regressor__regressor__nesterovs_momentum', 'estimator__regressor__regressor__power_t', 'estimator__regressor__regressor__random_state', 'estimator__regressor__regressor__shuffle', 'estimator__regressor__regressor__solver', 'estimator__regressor__regressor__tol', 'estimator__regressor__regressor__validation_fraction', 'estimator__regressor__regressor__verbose', 'estimator__regressor__regressor__warm_start', 'estimator__regressor', 'estimator__transformer', 'estimator', 'iid', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])\n",
      "Best params {'regressor__feature_selection__features': ['num_edges', 'num_vertices'], 'regressor__regressor__hidden_layer_sizes': (64, 64)}\n",
      "Mean Test score [-0.99546826 -0.94293733 -0.78461152 -0.36196225 -0.94212026 -0.85885556\n",
      " -0.35974262 -0.97713173 -0.91386992 -0.81710365 -0.45233388 -0.96345275\n",
      " -0.83864783 -0.40232913 -0.97686714 -0.91355181 -0.81588302 -0.45531638\n",
      " -0.96327487 -0.83822948 -0.39275673 -0.97578439 -0.95365749 -0.84018406\n",
      " -0.48468161 -0.94454761 -0.83212668 -0.37970081 -0.97729785 -0.91415022\n",
      " -0.81386263 -0.46155247 -0.9635447  -0.83968389 -0.39129498 -0.97515836\n",
      " -0.94810388 -0.83273138 -0.46403128 -0.94502881 -0.82810382 -0.37491378\n",
      " -0.9786167  -0.94806656 -0.83530657 -0.46618968 -0.94512716 -0.82854871\n",
      " -0.37677368 -0.9666625  -0.8972694  -0.63984869 -0.42399796 -0.96933466\n",
      " -0.83975907 -0.39737292 -0.9766264  -0.91736278 -0.84672794 -0.45574138\n",
      " -0.96106748 -0.8466826  -0.42473729 -0.97733281 -0.95721256 -0.84620959\n",
      " -0.49967244 -0.9491904  -0.85455471 -0.42388056 -0.97719136 -0.95716901\n",
      " -0.84546147 -0.49749064 -0.94907973 -0.85368149 -0.42122301 -0.97373291\n",
      " -0.91276411 -0.67017022 -0.48575741 -0.96318507 -0.86777661 -0.42507806\n",
      " -0.9771617  -0.95758776 -0.845414   -0.49255405 -0.94902667 -0.8548237\n",
      " -0.42377327 -0.9734218  -0.91328111 -0.66351199 -0.47613146 -0.96305495\n",
      " -0.86452518 -0.42596196 -0.97328248 -0.91342234 -0.66045511 -0.47330827\n",
      " -0.96325927 -0.86363584 -0.425362   -0.96988121 -0.86437814 -0.64926947\n",
      " -0.41718768 -0.95669838 -0.80313006 -0.42708226 -0.97558449 -0.91903378\n",
      " -0.82234673 -0.40203195 -0.96126379 -0.82648502 -0.37189194 -0.97844692\n",
      " -0.96207027 -0.86083483 -0.52267866 -0.95396669 -0.86002792 -0.38849519\n",
      " -0.97838337 -0.96213305 -0.86114344 -0.52194871 -0.95385989 -0.86036199\n",
      " -0.38389716 -0.97452031 -0.9190347  -0.69654393 -0.53254221 -0.96684882\n",
      " -0.88251646 -0.3759251  -0.97850008 -0.96257888 -0.86221628 -0.51850756\n",
      " -0.95395977 -0.86074398 -0.38415666 -0.9755449  -0.92146399 -0.69239522\n",
      " -0.5148153  -0.96646918 -0.88211634 -0.37262313 -0.97569259 -0.9213586\n",
      " -0.69329539 -0.51542119 -0.96659246 -0.88349172 -0.37460586 -0.97213733\n",
      " -0.87225493 -0.66515133 -0.38593319 -0.9586572  -0.82178242 -0.37990205\n",
      " -0.97911839 -0.96020261 -0.86154017 -0.54174093 -0.952118   -0.85581949\n",
      " -0.42133071 -0.97717664 -0.92938049 -0.72386432 -0.53882758 -0.97379052\n",
      " -0.87759107 -0.40728539 -0.97710151 -0.92995106 -0.72313991 -0.53747993\n",
      " -0.97368578 -0.87680028 -0.40593542 -0.97525995 -0.88875834 -0.68313344\n",
      " -0.43143733 -0.96343057 -0.84584878 -0.41415756 -0.97712296 -0.92358558\n",
      " -0.72216262 -0.53782073 -0.97389271 -0.87708561 -0.41174137 -0.97499335\n",
      " -0.88863929 -0.6805516  -0.42980634 -0.963374   -0.84484062 -0.41699999\n",
      " -0.97480175 -0.88845718 -0.67889766 -0.42730122 -0.9631777  -0.84449497\n",
      " -0.41540871 -0.97409939 -0.91739436 -0.68521715 -0.42112109 -0.95827003\n",
      " -0.79221123 -0.42661562]\n"
     ]
    }
   ],
   "source": [
    "overall_scores = []\n",
    "PATH_STORE_MODEL = helper.create_results_folder(\"/home/ubuntu/cephstorage/graph-processing-run-time-prediction-models\")\n",
    "PATH_STORE_SCORES = helper.create_results_folder(\"results-graph-processing-run-time-prediction\")\n",
    "\n",
    "for USE_ALL_PARTITIONER in [\n",
    "    True, \n",
    "  ]:\n",
    "  for algo in [\n",
    "    \"cc\", \n",
    "    \"pr\",\n",
    "    \"sssp1\", \n",
    "    \"kcoreavg\",\n",
    "    \"synthetic1c0\",\n",
    "    \"synthetic10c0\"\n",
    "    ]:\n",
    "    for model_name in [\n",
    "      #\"KNN\",\n",
    "      #\"PolyRegression\",\n",
    "      #\"XGB\", \n",
    "      #\"RFR\", \n",
    "      #\"SVR\",\n",
    "      \"MLP\"\n",
    "      ]:\n",
    "      print(\"We start the training with model ({}) for algorithm ({}). We build ONE model for all partitioners: {}\".format(model_name, algo, USE_ALL_PARTITIONER))\n",
    "      if USE_ALL_PARTITIONER:\n",
    "        training_approach = \"single-model-for-all-partitioners\"\n",
    "        # We use all the partitioners for the paper\n",
    "        _X_train =  X_train[(X_train.partitioner.isin(paper_partitioners)) &(X_train.algorithm == algo)]\n",
    "        _y_train =  y_train[(y_train.partitioner.isin(paper_partitioners)) &(y_train.algorithm == algo)]\n",
    "        _X_test =  X_test[(X_test.partitioner.isin(paper_partitioners)) &(X_test.algorithm == algo)]\n",
    "        _y_test =  y_test[(y_test.partitioner.isin(paper_partitioners)) &(y_test.algorithm == algo)]\n",
    "     #   print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_y_train with {} samples. {} are expected\".format(len(_y_train), NUM_TRAIN_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_X_test with {} samples. {} are expected\".format(len(_X_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "     #   print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS * NUM_PARTITIONERS))\n",
    "        # We get the model which includes the gridsearch and pipeline\n",
    "        model = helper.get_model(model_name, featuresets)\n",
    "        # Get scores: Will include that crossvalidation score for the best hyperparameters and also the testscore for the best hyperparameters. \n",
    "        # The testscores will be 1) per partitioner and per graph 2) per graph 3) per partitioner 4) overall score\n",
    "        scores = train(\n",
    "          model_name=model_name, \n",
    "          algorithm=algo, \n",
    "          model=model, \n",
    "          X_train=_X_train, \n",
    "          y_train=_y_train, \n",
    "          X_test=_X_test, \n",
    "          y_test=_y_test, \n",
    "          feature_combinations=featuresets, \n",
    "          target=TARGET, \n",
    "          store_model=True,\n",
    "          result_file_name=\"{}/{}_{}_{}\".format(PATH_STORE_MODEL, training_approach, algo, model_name))\n",
    "\n",
    "\n",
    "        for score in scores:\n",
    "          score[\"algorithm\"] = algo\n",
    "          score[\"training_approach\"] = training_approach\n",
    "        overall_scores += scores\n",
    "        pd.DataFrame(scores).to_csv(\"{}/{}_{}_{}\".format(PATH_STORE_SCORES, algo, training_approach, model_name))\n",
    "      else:\n",
    "        training_approach = \"single-model-per-partitioner\"\n",
    "        for partitioner in paper_partitioners:\n",
    "            _X_train =  X_train[(X_train.partitioner == partitioner) &(X_train.algorithm == algo)]\n",
    "            _y_train =  y_train[(y_train.partitioner == partitioner)&(y_train.algorithm == algo)]\n",
    "            _X_test =  X_test[(X_test.partitioner == partitioner)&(X_test.algorithm == algo)]\n",
    "            _y_test =  y_test[(y_test.partitioner == partitioner)&(y_test.algorithm == algo)]\n",
    "        #    print(\"_X_train with {} samples. {} are expected\".format(len(_X_train), NUM_TRAIN_GRAPHS ))\n",
    "        #    print(\"_y_train with {} samples. {} are expected\".format(len(_y_train), NUM_TRAIN_GRAPHS ))\n",
    "        #    print(\"_X_test with {} samples. {} are expected\".format(len(_X_test),  NUM_TEST_GRAPHS))\n",
    "        #    print(\"_y_test with {} samples. {} are expected\".format(len(_y_test), NUM_TEST_GRAPHS))\n",
    "            # We get the model which includes the gridsearch and pipeline\n",
    "            model = helper.get_model(model_name, featuresets)\n",
    "            # Get scores: Will include that crossvalidation score for the best hyperparameters and also the testscore for the best hyperparameters. \n",
    "            # The testscores will be 1) per partitioner and per graph 2) per graph 3) per partitioner 4) overall score\n",
    "            scores = train(\n",
    "              model_name=model_name, \n",
    "              algorithm=algo , \n",
    "              model=model, \n",
    "              X_train=_X_train, \n",
    "              y_train=_y_train, \n",
    "              X_test=_X_test,\n",
    "              y_test=_y_test, \n",
    "              feature_combinations=featuresets,\n",
    "              target=TARGET,\n",
    "              store_model=True,\n",
    "              result_file_name=\"{}/{}_{}_{}_{}\".format(PATH_STORE_MODEL, partitioner, training_approach, algo, model_name))\n",
    "            for score in scores:\n",
    "              score[\"algorithm\"] = algo\n",
    "              score[\"training_approach\"] = training_approach\n",
    "            overall_scores += scores\n",
    "            pd.DataFrame(scores).to_csv(\"{}/{}_{}_{}_{}\".format(PATH_STORE_SCORES, training_approach, algo, partitioner, model_name))\n",
    "\n",
    "scores_df = pd.DataFrame(overall_scores)\n",
    "scores_df.to_csv(\"{}/scores-{}\".format(PATH_STORE_SCORES, TARGET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluations\n",
    "Have a look at the run-time-prediction-scores.ipynb with the scores reported in the paper \n",
    "\n",
    "set the right timestamp!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(\"{}/scores-processing_time\".format(PATH_STORE_SCORES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We trained 12 models but select the best one based on crossvalidation on the rmat graphs\n",
      "One average we can predict the grap processing run-time with a mape of 0.315802392250061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mape</th>\n",
       "      <th>partitioner</th>\n",
       "      <th>graph</th>\n",
       "      <th>model</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>rmat_cv_score</th>\n",
       "      <th>training_approach</th>\n",
       "      <th>rmat_best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.828745</td>\n",
       "      <td>35304.401606</td>\n",
       "      <td>0.272462</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>cc</td>\n",
       "      <td>-0.199110</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.567524</td>\n",
       "      <td>167424.679961</td>\n",
       "      <td>0.370609</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>kcoreavg</td>\n",
       "      <td>-0.368741</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.680508</td>\n",
       "      <td>9054.331592</td>\n",
       "      <td>0.313933</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>pr</td>\n",
       "      <td>-0.162552</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>0.916665</td>\n",
       "      <td>59542.297729</td>\n",
       "      <td>0.407423</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>sssp1</td>\n",
       "      <td>-0.141117</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['vertex_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.785240</td>\n",
       "      <td>14193.292422</td>\n",
       "      <td>0.259128</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>synthetic10c0</td>\n",
       "      <td>-0.181267</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.742168</td>\n",
       "      <td>13227.444964</td>\n",
       "      <td>0.271258</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>PolyRegression</td>\n",
       "      <td>synthetic1c0</td>\n",
       "      <td>-0.175891</td>\n",
       "      <td>single-model-for-all-partitioners</td>\n",
       "      <td>{'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           r2           rmse      mape partitioner graph           model  \\\n",
       "88   0.828745   35304.401606  0.272462         all   all  PolyRegression   \n",
       "622  0.567524  167424.679961  0.370609         all   all  PolyRegression   \n",
       "266  0.680508    9054.331592  0.313933         all   all  PolyRegression   \n",
       "444  0.916665   59542.297729  0.407423         all   all  PolyRegression   \n",
       "978  0.785240   14193.292422  0.259128         all   all  PolyRegression   \n",
       "800  0.742168   13227.444964  0.271258         all   all  PolyRegression   \n",
       "\n",
       "         algorithm  rmat_cv_score                  training_approach  \\\n",
       "88              cc      -0.199110  single-model-for-all-partitioners   \n",
       "622       kcoreavg      -0.368741  single-model-for-all-partitioners   \n",
       "266             pr      -0.162552  single-model-for-all-partitioners   \n",
       "444          sssp1      -0.141117  single-model-for-all-partitioners   \n",
       "978  synthetic10c0      -0.181267  single-model-for-all-partitioners   \n",
       "800   synthetic1c0      -0.175891  single-model-for-all-partitioners   \n",
       "\n",
       "                                                                                                                                                                                             rmat_best_params  \n",
       "88   {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  \n",
       "622  {'regressor__feature_selection__features': ['vertex_balance', 'destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  \n",
       "266                    {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  \n",
       "444                         {'regressor__feature_selection__features': ['vertex_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  \n",
       "978                    {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  \n",
       "800                    {'regressor__feature_selection__features': ['destination_balance', 'source_balance', 'replication_factor', 'edge_balance', 'num_edges', 'num_vertices'], 'regressor__poli__degree': 2}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_overall_df = scores_df[(scores_df.graph == \"all\") & (scores_df.partitioner == \"all\") & (scores_df.training_approach == \"single-model-for-all-partitioners\")  ]\n",
    "print(\"We trained {} models but select the best one based on crossvalidation on the rmat graphs\".format(len(scores_overall_df)))\n",
    "\n",
    "idx = scores_overall_df.groupby([\"algorithm\", \"partitioner\"])['rmat_cv_score'].transform(max) == scores_overall_df['rmat_cv_score']\n",
    "best_scores_overall_df = scores_overall_df[idx].sort_values(by=[\"algorithm\", \"mape\"])\n",
    "print(\"One average we can predict the grap processing run-time with a mape of {}\".format(best_scores_overall_df[\"mape\"].mean()))\n",
    "best_scores_overall_df[[\"r2\", \"rmse\", \"mape\", \"partitioner\", \"graph\", \"model\", \"algorithm\", \"rmat_cv_score\", \"training_approach\", \"rmat_best_params\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
